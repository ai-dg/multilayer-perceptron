# Exercices Python â€” Linear Algebra (Goodfellow, Chapitre 2)

> ConventionÂ : texte explicatif en franÃ§ais, **termes techniques en anglais britannique** (matrix, vector, Hadamard product, QR decomposition, orthonormal, orthogonal, normalise, eigen, etc.).

---

## 2.1 â€” Scalars, Vectors, Matrices and Tensors

### Exercice 1

**Ã‰noncÃ© :** 
CrÃ©er un *scalar*, un *vector* 1â€‘D, une *matrix* 2â€‘D et un *tensor* 3â€‘D.  

**Instructions :**
- Choisir des `dtype` explicites (ex. `np.float32`).  
- Afficher pour chacunÂ : `value`, `np.ndim`, `np.shape`, `dtype`.  
- (Optionnel) vÃ©rifier que `scalar` a `shape == ()`.  

**Fonctions utiles :** 
- `np.array`.
- `np.asarray`.
- `np.random.rand`.
- `np.ndim`.
- `np.shape`.

---

## 2.2 â€” Matrix and Vector Operations

### Exercice 1 â€” Addition de vecteurs

**Ã‰noncÃ© :** 
Additionner deux *vectors* de mÃªme taille, Ã©lÃ©ment par Ã©lÃ©ment.  

**Instructions :** 
- VÃ©rifie que les deux vecteurs ont la mÃªme forme.
- Utilise une mÃ©thode explicite (pas `a + b` directement).

**Fonctions utiles :** 
- `np.add`.
- `np.shape`.

---

### Exercice 2 â€” Hadamard product

**Ã‰noncÃ© :** 
Multiplier deux *matrices* de mÃªme forme, Ã©lÃ©ment par Ã©lÃ©ment (â‰  produit matriciel).  

**Instructions :** 
- VÃ©rifie la compatibilitÃ© des dimensions.
- Applique une multiplication Ã©lÃ©ment-par-Ã©lÃ©ment (â‰  produit matriciel).

**Fonctions utiles :** 
- `np.asarray`
- `np.multiply`
- `np.shape`

---

### Exercice 3 â€” Centrage dâ€™un vecteur

**Ã‰noncÃ© :** 
Centrer un *vector* en retirant sa moyenne.  

**Instructions :** 
- Convertis le vecteur si besoin.
- Calcule la moyenne du vecteur.
- Soustrais la moyenne Ã  chaque Ã©lÃ©ment.

**Fonctions utiles :** 
- `np.mean`
- `np.asarray`

---

### Exercice 4 â€“ VÃ©rification du centrage

**Ã‰noncÃ© :** 
VÃ©rifie que la somme des Ã©lÃ©ments du vecteur centrÃ© est (presque) nulle.

**Instructions :** 
- Calcule la somme du vecteur centrÃ©.
- Compare avec 0 avec une tolÃ©rance.

**Fonctions utiles :** 
- `np.sum`
- `np.abs`

---
## 2.3 â€” Multiplying Matrices and Vectors

### Exercice 1 â€” Produit matrice-vecteur

**Ã‰noncÃ© :**  
Ã‰cris une fonction qui effectue le produit $y = A \times x$ entre une matrice $A$ et un vecteur $x$, **sans utiliser** `@` ni `np.dot`.

**Instructions :**
- ImplÃ©mente-le Ã  la main avec des boucles `for` et des sommes.
- VÃ©rifie que les dimensions sont compatibles (`A.shape[1] == x.shape[0]`).

**Fonctions utiles :**  
`np.shape`, `np.asarray`

---

### Exercice 2 â€” VÃ©rification de lâ€™associativitÃ©

**Ã‰noncÃ© :**  
VÃ©rifie numÃ©riquement queÂ :  
$$(A @ B) @ x \approx A @ (B @ x)$$

**Instructions :**
- CrÃ©e trois matrices/vecteurs compatibles (`A`, `B`, `x`).
- Calcule les deux cÃ´tÃ©s.
- Compare les rÃ©sultats avec une tolÃ©rance.

**Fonctions utiles :**  
`np.allclose`, `np.matmul` (ou `@`), `np.linalg.norm`

---

### Exercice 3 â€” VÃ©rification de la distributivitÃ©

**Ã‰noncÃ© :**  
VÃ©rifie queÂ :  
$$A @ (x + y) \approx A @ x + A @ y$$

**Instructions :**
- CrÃ©e `A`, `x`, `y` compatibles.
- Calcule les deux membres.
- Compare les rÃ©sultats.

**Fonctions utiles :**  
`np.add`, `np.matmul`, `np.allclose`

---

## 2.4 â€” Identity and Inverse Matrices

**Objectif :**  
Explorer les matrices identitÃ© et inverses avec NumPy.

**Instructions :**
- CrÃ©er une matrice carrÃ©e `A` (3Ã—3) avec des `float32` au hasard (ex. `np.random.rand(3, 3)`) ou une matrice fixe.
- CrÃ©er une matrice identitÃ© `I` de mÃªme taille que `A` avec `np.eye(...)`.
- VÃ©rifie que $A \times I = A$ (produit matriciel avec `np.matmul` ou `@`) pour valider le rÃ´le de la matrice identitÃ©.
- Essaie dâ€™inverser `A` avec `np.linalg.inv(A)`Â :
    - Si lâ€™inverse existe, stocke-le dans `A_inv`.
    - VÃ©rifie que $A @ A_{inv} \approx I$ avec `np.allclose(...)`.
    - Affiche la matrice inverse et le rÃ©sultat de la multiplication $A @ A_{inv}$.
- *(Optionnel)* GÃ¨re lâ€™exception si `A` est non inversible (`LinAlgError`) avec un `try/except`.

**Fonctions / objets Ã  utiliser :**  
`np.eye`, `np.linalg.inv`, `np.matmul` ou `@`, `np.allclose`  
*(optionnel)* `try` + `except np.linalg.LinAlgError`

---

## 2.5 â€” Linear Dependence and Span

### Exercice 1 â€” Rang dâ€™une matrice

**Ã‰noncÃ© :**  
Calcule le rang dâ€™une matrice `A` et interprÃ¨te ce que Ã§a signifie en termes de dÃ©pendance linÃ©aire.

**Instructions :**
- CrÃ©e une matrice `A` avec certaines colonnes dÃ©pendantes.
- Utilise la fonction de rang.
- VÃ©rifieÂ : si `rank(A) < n_cols`, les colonnes sont linÃ©airement dÃ©pendantes.

**Fonctions utiles :**  
`np.linalg.matrix_rank`

---

### Exercice 2 â€” Vecteur dans le span

**Ã‰noncÃ© :**  
Teste si un vecteur `v` appartient au span des colonnes dâ€™une matrice `B`.

**Instructions :**
- RÃ©sous le systÃ¨me $B @ x \approx v$ (mÃ©thode des moindres carrÃ©s).
- VÃ©rifie si la norme du rÃ©sidu est â‰ˆ 0.

**Fonctions utiles :**  
`np.linalg.lstsq`, `np.allclose` ou `np.linalg.norm`

---

### Exercice 3 â€” DÃ©pendance explicite

**Ã‰noncÃ© :**  
Montre explicitement une combinaison linÃ©aire non triviale entre des vecteurs.

**Instructions :**
- Construis 3 vecteurs en 2D (par ex. $(1,0)$, $(0,1)$, $(1,1)$).
- Montre quâ€™il existe des coefficients $c_1, c_2, c_3$ (pas tous nuls) tels que  
    $c_1 \cdot v_1 + c_2 \cdot v_2 + c_3 \cdot v_3 = 0$.

**Fonctions utiles :**  
`np.array`, `np.linalg.matrix_rank` (pour confirmer la dÃ©pendance)

---

## 2.6 â€” Norms

### Exercice 1 â€” Láµ– norm (implÃ©mentation)

**Ã‰noncÃ© :** 

ImplÃ©menter `lp_norm(x, p)` pour `p â‰¥ 1` et comparer Ã  `np.linalg.norm`.  

**Instructions :** 
- Ã‰cris ta propre fonction `lp_norm(x, p)`.
- Compare avec `np.linalg.norm(x, ord=p)`.

**Fonctions utiles :** 
- `np.abs`
- `np.sum`
- `np.power`
- `np.max`
- `np.linalg.norm`.

---

### Exercice 2 â€” Comparaison LÂ¹, LÂ², Lâˆ

**Ã‰noncÃ© :** 
Pour un mÃªme vecteur, calcule et compare ses normes :
- LÂ¹ (somme des valeurs absolues)
- LÂ² (distance euclidienne)
- Lâˆ (valeur absolue maximale)

**Instructions :** 
- CrÃ©e un vecteur x.
- Calcule et affiche chaque norme.
- Explique la diffÃ©rence gÃ©omÃ©trique entre elles.

**Fonctions utiles :** 
- `np.linalg.norm (ord=1, 2, np.inf)`.
- `np.max`.

---

### Exercice 3 â€” Normalisation de vectors

**Ã‰noncÃ© :** 
Normaliser chaque *row vector* par sa LÂ² norm.  

**Instructions :** 
- CrÃ©e une matrice 2D (plusieurs vecteurs ligne).
- Normalise chaque vecteur ligne.
- GÃ¨re le cas oÃ¹ la norme est 0 (Ã©vite division par 0).

**Fonctions utiles :** 
- `np.linalg.norm(axis=1, keepdims=True)`. 
- `np.where`, diffusion (broadcasting).

---

## 2.7 â€” Special Kinds of Matrices and Vectors

### Exercice 1 â€” Symmetric / Skewâ€‘symmetric

**Ã‰noncÃ© :** 
Tester si `A` est **symmetric** (`A â‰ˆ Aáµ€`) ou **skewâ€‘symmetric** (`A â‰ˆ âˆ’Aáµ€`).  

**Instructions :**
- CrÃ©e une matrice carrÃ©e `A`.
- VÃ©rifie sÃ©parÃ©ment `A â‰ˆ A.T` et `A â‰ˆ -A.T` (mÃªme tolÃ©rance).

**Fonctions utiles :** 
- `np.allclose`.
- `A.T` (transpose).
- (optionnel) construction de `A_sym = (A + A.T)/2`, `A_skew = (A - A.T)/2`.

---

### Exercice 2 â€” Diagonal matrix et application efficace

**Ã‰noncÃ© :**
Ã€ partir dâ€™un vecteur d, construis `D = diag(d)` et compare `D @ x` avec le produit Ã©lÃ©ment-par-Ã©lÃ©ment `d * x`.

**Instructions :**
- CrÃ©e `d` (1-D) et un vecteur x compatible.
- Construis la diagonal matrix Ã  partir de `d`.
- Calcule `D @ x` et `d * x` et vÃ©rifie lâ€™Ã©galitÃ© (â‰ˆ).

**Fonctions utiles :** 
- `np.diag`.
- `@` ou `np.matmul`.
- `np.allclose`

---

### Exercice 3 â€” Orthonormal basis via QR

**Ã‰noncÃ© :**
Construis une orthonormal basis `Q` Ã  partir dâ€™une matrice pleine colonne via QR decomposition et vÃ©rifie `Qáµ€Q â‰ˆ I`.

**Instructions :** 
- CrÃ©e une matrice `A` (mÃ—n, rang plein, mâ‰¥n).
- Fais `Q, R = np.linalg.qr(A)` (mode rÃ©duit par dÃ©faut).
- VÃ©rifie `Q.T @ Q â‰ˆ Iâ‚™` et que `R` est **upper-triangular** (tolÃ©rance sur les Ã©lÃ©ments sous-diagonaux).

**Fonctions utiles :** 
- `np.linalg.qr`.
- `np.eye`.
- `np.tril`, `np.allclose`.

---

### Exercice 4 â€” Orthogonal matrix

**Ã‰noncÃ© :**
Teste si une matrice `Q` est orthogonal (colonnes orthonormÃ©es) : `Qáµ€Q â‰ˆ I` et `Qâ»Â¹ â‰ˆ Qáµ€`.  

**Instructions :**
- Utilise `Q` issu de lâ€™Ex.3 (ou construis-en un).
- Calcule `Q.T @ Q` et compare Ã  `I`.
- Calcule `np.linalg.inv(Q)` et compare Ã  `Q.T`.

**Fonctions utiles :** 
- `np.allclose`. 
- `np.linalg.inv`.
- `np.eye`.

---

### Exercice 5 â€” Orthogonality & unit vectors

**Ã‰noncÃ© :**
VÃ©rifie lâ€™orthogonality de deux vecteurs `u, v` (`uáµ€v â‰ˆ 0`) et la propriÃ©tÃ© unit norm (`â€–uâ€–â‚‚ = 1`).

**Instructions :**
- Normalise `u` et `v` si nÃ©cessaire.
- Calcule `u.T @ v` et `np.linalg.norm(u, 2)`.
- Conclus sur orthogonalitÃ© et norme unitaire.

**Fonctions utiles :** 
- `np.linalg.norm`.
- `np.dot` ou `@`.
- (optionnel) `u / np.linalg.norm(u)`

---

# 2.8 â€” Eigendecomposition

## Exercice 1 â€” Calcul des eigenvalues et eigenvectors

**Ã‰noncÃ© :**  
Ã‰tant donnÃ© une matrice carrÃ©e `A`, calcule ses **eigenvalues** et **eigenvectors**.

**Instructions :**
- Construire une matrice carrÃ©e `A` (ex. 3Ã—3).  
- Utiliser `np.linalg.eig(A)` pour obtenir les `Î»` et les `v`.  
- Pour chaque couple `(Î»áµ¢, váµ¢)`, vÃ©rifier que `A @ váµ¢ â‰ˆ Î»áµ¢ Â· váµ¢`.  
- Utiliser une tolÃ©rance (`np.allclose`) car les calculs sont numÃ©riques.

**Fonctions utiles :**
- `np.linalg.eig(A)`  
- `np.allclose`, `@`  

---

## Exercice 2 â€” Reconstruction via eigendecomposition

**Ã‰noncÃ© :**  
Recompose `A` Ã  partir de ses eigenvectors et eigenvalues.

**Instructions :**
- Appelle `eigvals, eigvecs = np.linalg.eig(A)`  
- Forme `Î› = np.diag(eigvals)`  
- Forme `V = eigvecs` (matrice colonnes)  
- Calcule `V @ Î› @ Vâ»Â¹` et compare Ã  `A`  
- VÃ©rifie `A â‰ˆ V @ Î› @ Vâ»Â¹` avec tolÃ©rance

**Fonctions utiles :**
- `np.diag`, `np.linalg.inv`, `np.allclose`  

---

## Exercice 3 â€” Eigendecomposition de matrices symÃ©triques

**Ã‰noncÃ© :**  
Pour toute matrice rÃ©elle et symÃ©trique, les eigenvectors sont **orthonormaux**.

**Instructions :**
- Construire une matrice symÃ©trique `A` (ex. `A = A + A.T`)  
- Utiliser `np.linalg.eigh(A)` (optimisÃ© pour les matrices symÃ©triques)  
- RÃ©cupÃ©rer `Q = eigvecs`  
- VÃ©rifier que `Q.T @ Q â‰ˆ I`

**Fonctions utiles :**
- `np.linalg.eigh`  
- `np.eye`, `np.allclose`

---

## Exercice 4 â€” DÃ©tecter si une matrice est singuliÃ¨re

**Ã‰noncÃ© :**  
Une matrice est **singular** (non inversible) ssi un eigenvalue est â‰ˆ 0.

**Instructions :**
- Calculer les `eigvals` de `A`  
- Tester `np.any(np.isclose(eigvals, 0.0))`  
- Si oui â†’ `A` est singuliÃ¨re.

**Fonctions utiles :**
- `np.linalg.eig`, `np.isclose`, `np.any`  

---

## Exercice 5 â€” Quadratic form et eigenvalue maximale

**Ã‰noncÃ© :**  
ConsidÃ¨re `f(x) = xáµ€ A x` avec `â€–xâ€–â‚‚ = 1`.  
Le **maximum de f(x)** est atteint pour `x = eigenvector_max` et vaut `Î»_max`.

**Instructions :**
- Construire une matrice symÃ©trique `A`  
- Calculer ses eigenvalues avec `np.linalg.eigh`  
- GÃ©nÃ©rer plusieurs vecteurs `x` alÃ©atoires normalisÃ©s  
- Calculer `f(x)` pour chacun  
- VÃ©rifier que `max(f(x)) â‰ˆ max(eigvals)`

**Fonctions utiles :**
- `np.random.randn`, `np.linalg.norm`, `np.max`  
- `x.T @ A @ x` ou `np.dot(x, A @ x)`

---

# 2.9 â€” Singular Value Decomposition (SVD)

> On utilise la factorisation matricielle suivante :  
> **A = U Â· Î£ Â· Váµ€**, oÃ¹ :
> - `U` est une matrice orthogonale (colonnes = left singular vectors)
> - `Î£` est diagonale avec les **singular values**
> - `Váµ€` contient les right singular vectors (transposÃ©s)

---

## Exercice 1 â€” DÃ©composer une matrice A en U, Î£, Váµ€

**Ã‰noncÃ© :**  
Effectuer la dÃ©composition SVD dâ€™une matrice rÃ©elle `A`.

**Instructions :**
- Construire une matrice `A` (pas nÃ©cessairement carrÃ©e).
- Utiliser `np.linalg.svd(A) â†’ U, S, Vt`
- Afficher les formes (`shape`) de `U`, `S`, `Vt` et vÃ©rifier la relation `A â‰ˆ U Â· Î£ Â· Váµ€`

**Fonctions utiles :**
- `np.linalg.svd(A)`  
- `np.diag(S)` ou `np.zeros(A.shape)` + `np.fill_diagonal()`  
- `@`, `np.allclose`

---

## Exercice 2 â€” InterprÃ©tation gÃ©omÃ©trique des singular values

**Ã‰noncÃ© :**  
Les singular values reprÃ©sentent le **facteur dâ€™Ã©tirement** maximal de `A` selon chaque direction.

**Instructions :**
- GÃ©nÃ©rer plusieurs vecteurs `x` normÃ©s alÃ©atoires (â€–xâ€–â‚‚ = 1)
- Calculer `â€–A @ xâ€–â‚‚` pour chacun
- Montrer que le maximum de ces normes est â‰ˆ `S[0]` (plus grande singular value)

**Fonctions utiles :**
- `np.random.randn`, `np.linalg.norm`, `np.max`  
- `np.linalg.svd`  

---

## Exercice 3 â€” Reconstruction approchÃ©e de A

**Ã‰noncÃ© :**  
Reconstituer une **approximation de rang k** de `A` Ã  partir de ses premiers vecteurs singuliers.

**Instructions :**
- Prendre les `k` premiers vecteurs de `U`, `S`, `Vt`
- Construire `A_k = U_k @ Î£_k @ Vt_k`
- Comparer `A_k` Ã  `A`

**Fonctions utiles :**
- slicing `U[:, :k]`, `S[:k]`, `Vt[:k, :]`  
- `np.diag` ou `np.diagflat` pour `Î£_k`

---

## Exercice 4 â€” Compression par rÃ©duction de rang

**Ã‰noncÃ© :**  
Comparer la taille mÃ©moire de `A` et de sa version approchÃ©e `A_k`.

**Instructions :**
- Calculer `size_A = mÂ·n`
- Calculer `size_Ak = mÂ·k + k + kÂ·n` (pour stocker `U_k`, `S_k`, `Vt_k`)
- Afficher le taux de compression

**Fonctions utiles :**
- `np.prod(A.shape)`, opÃ©rations simples

---

## Exercice 5 â€” RÃ©duction de bruit (denoising)

**Ã‰noncÃ© :**  
Appliquer SVD Ã  une matrice bruitÃ©e et reconstruire une version "propre".

**Instructions :**
- Ajouter du bruit Ã  une matrice `A` (`A_noisy = A + noise`)
- Faire la SVD de `A_noisy`
- Recomposer `A_clean` avec les premiers `k` composants
- Comparer `A_clean` et `A`

**Fonctions utiles :**
- `np.random.normal(scale=Ïƒ)`, `np.linalg.svd`
- reconstruction avec `k` premiers composants
- `np.linalg.norm(A_clean - A)`

---

# 2.10 â€” The Moore-Penrose Pseudoinverse

> La pseudoinverse est une gÃ©nÃ©ralisation de lâ€™inverse matriciel pour les matrices non carrÃ©es ou singuliÃ¨res.  
> NotÃ©e `Aâº`, elle permet de rÃ©soudre des systÃ¨mes `Ax = b` mÃªme lorsque `A` nâ€™est pas inversible.

---

## Exercice 1 â€” Calcul de la pseudoinverse

**Ã‰noncÃ© :**  
Calculer la **Mooreâ€“Penrose pseudoinverse** dâ€™une matrice rectangulaire `A`.

**Instructions :**
- CrÃ©e une matrice `A` (non carrÃ©e ou de rang incomplet).
- Calcule `A_pinv = np.linalg.pinv(A)`.
- VÃ©rifie la relation de reconstruction : `A @ A_pinv @ A â‰ˆ A` et `A_pinv @ A @ A_pinv â‰ˆ A_pinv`.

**Fonctions utiles :**
- `np.linalg.pinv`, `np.allclose`, `@`

---

## Exercice 2 â€” RÃ©solution de systÃ¨mes linÃ©aires

**Ã‰noncÃ© :**  
Utiliser la pseudoinverse pour rÃ©soudre un systÃ¨me non inversible.

**Instructions :**
- Soit `A` une matrice non carrÃ©e (ex: 3Ã—2), et `b` un vecteur (3Ã—1)
- RÃ©sous `x = Aâº @ b`
- VÃ©rifie la solution `Ax â‰ˆ b` avec tolÃ©rance

**Fonctions utiles :**
- `np.linalg.pinv`, `np.allclose`, `np.dot`

---

## Exercice 3 â€” Cas sous-dÃ©terminÃ© vs sur-dÃ©terminÃ©

**Ã‰noncÃ© :**  
Ã‰tudier le comportement de la pseudoinverse sur des systÃ¨mes :
- **Sous-dÃ©terminÃ©** : plus de variables que dâ€™Ã©quations
- **Sur-dÃ©terminÃ©** : plus dâ€™Ã©quations que de variables

**Instructions :**
- Construire `Aâ‚` (2Ã—3) sous-dÃ©terminÃ©, et `Aâ‚‚` (4Ã—2) sur-dÃ©terminÃ©
- GÃ©nÃ©rer des vecteurs `bâ‚`, `bâ‚‚`
- RÃ©soudre avec `Aâº @ b`
- Comparer les rÃ©sidus `â€–Ax - bâ€–`

**Fonctions utiles :**
- `np.linalg.pinv`, `np.linalg.norm`

---

## Exercice 4 â€” Comparaison avec lstsq

**Ã‰noncÃ© :**  
Comparer la solution par pseudoinverse avec celle de `np.linalg.lstsq`.

**Instructions :**
- Pour une matrice `A` non inversible et vecteur `b`, calcule :
  - `x_pinv = Aâº @ b`
  - `x_lstsq = np.linalg.lstsq(A, b)[0]`
- Compare les deux vecteurs (et leur norme)

**Fonctions utiles :**
- `np.linalg.pinv`, `np.linalg.lstsq`, `np.allclose`

---

## Exercice 5 â€” Pseudoinverse via SVD

**Ã‰noncÃ© :**  
ImplÃ©menter manuellement la pseudoinverse via **SVD**.

**Instructions :**
- Effectuer `U, S, Vt = np.linalg.svd(A)`
- Inverser `S` : `Sâº = 1/S` (sauf zÃ©ros)
- Construire `Aâº = Vt.T @ diag(Sâº) @ U.T`
- Comparer Ã  `np.linalg.pinv(A)`

**Fonctions utiles :**
- `np.linalg.svd`, `np.diag`, `np.linalg.pinv`, `np.allclose`
- `np.where`, `np.divide`

---

# 2.11 â€” The Trace Operator

> Le **trace** dâ€™une matrice carrÃ©e `A` est la somme de ses Ã©lÃ©ments diagonaux.  
> Elle possÃ¨de des propriÃ©tÃ©s fondamentales utiles en algÃ¨bre linÃ©aire et en apprentissage automatique.

---

## Exercice 1 â€” Calcul de la trace

**Ã‰noncÃ© :**  
Calculer la trace dâ€™une matrice carrÃ©e `A`.

**Instructions :**
- CrÃ©er une matrice `A` de taille `nÃ—n`
- Utiliser `np.trace(A)` pour obtenir sa trace
- VÃ©rifier le rÃ©sultat manuellement avec `sum(A[i, i])`

**Fonctions utiles :**
- `np.trace`, slicing `[i, i]`, `np.sum`

---

## Exercice 2 â€” PropriÃ©tÃ© : Tr(A + B) = Tr(A) + Tr(B)

**Ã‰noncÃ© :**  
VÃ©rifier que la trace est linÃ©aire.

**Instructions :**
- CrÃ©er deux matrices carrÃ©es `A` et `B` de mÃªme dimension
- Calculer `Tr(A) + Tr(B)` et `Tr(A + B)`
- VÃ©rifier que les deux valeurs sont Ã©gales

**Fonctions utiles :**
- `np.trace`, `np.allclose`

---

## Exercice 3 â€” PropriÃ©tÃ© : Tr(AB) = Tr(BA)

**Ã‰noncÃ© :**  
VÃ©rifier que la trace est **invariante par permutation cyclique** (si les dimensions permettent).

**Instructions :**
- CrÃ©er deux matrices `A (nÃ—m)` et `B (mÃ—n)`
- Calculer `Tr(AB)` et `Tr(BA)`
- Comparer les deux rÃ©sultats

**Fonctions utiles :**
- `@`, `np.trace`, `np.allclose`

---

## Exercice 4 â€” Trace et produit scalaire

**Ã‰noncÃ© :**  
Montrer que pour deux matrices `A` et `B` de mÃªme taille :

```math
Tr(Aáµ€B) = âˆ‘ A_ij Â· B_ij = âŸ¨A, BâŸ©
```
Câ€™est-Ã -dire que Tr(Aáµ€B) donne le produit scalaire matriciel.

**Instructions :**
- CrÃ©er deux matrices `A`, `B` de mÃªme dimension
- Calculer `Tr(Aáµ€B)` et `np.sum(A * B)`
- VÃ©rifier que les deux sont Ã©gaux

**Fonctions utiles :**
- `np.trace`, `np.sum`, `*, @, .T`

---

## Exercice 5 - Trace et invariance orthogonale

**Ã‰noncÃ© :**

VÃ©rifier que la trace est invariante par transformation orthogonale :

```math
T_r(Q^TAQ) = T_r(A)
```
Si `Q` est orthogonale `(Qáµ€Q = I)`

**Instructions :**

- CrÃ©er une matrice carrÃ©e `A`
- GÃ©nÃ©rer une matrice orthogonale Q (ex: via `np.linalg.qr`)
- Calculer `Tr(Qáµ€ A Q)` et `Tr(A)`
- VÃ©rifier leur Ã©galitÃ©

**Fonctions utiles :**
- `np.linalg.qr`, `@`, `np.trace`, `np.allclose`

---

# 2.12 â€” The Determinant

> Le **dÃ©terminant** est une fonction scalaire appliquÃ©e aux matrices carrÃ©es, qui reflÃ¨te :
> - Le **volume** transformÃ© par la matrice
> - La **singularitÃ©** (inversibilitÃ©)
> - Le **changement dâ€™orientation**

---

## Exercice 1 â€” Calcul du dÃ©terminant

**Ã‰noncÃ© :**  
Calculer le dÃ©terminant d'une matrice carrÃ©e `A`.

**Instructions :**
- CrÃ©er une matrice `A` (ex: 2Ã—2 ou 3Ã—3)
- Utiliser `np.linalg.det(A)`
- VÃ©rifier manuellement sur de petits cas simples (ex: matrice diagonale ou triangulaire)

**Fonctions utiles :**
- `np.linalg.det`, `np.diag`, `np.tril`, `np.triu`

---

## Exercice 2 â€” DÃ©terminant et inversibilitÃ©

**Ã‰noncÃ© :**  
VÃ©rifier si une matrice est inversible Ã  partir de son dÃ©terminant.

**Instructions :**
- CrÃ©er une matrice `A`
- Calculer `det(A)`
- Si `|det(A)| > 0`, elle est inversible
- Si `det(A) = 0`, elle est singuliÃ¨re

**Fonctions utiles :**
- `np.linalg.det`, `np.isclose`, `np.linalg.inv`

---

## Exercice 3 â€” Effet dâ€™un swap de lignes

**Ã‰noncÃ© :**  
VÃ©rifier que lâ€™Ã©change de deux lignes change le **signe** du dÃ©terminant.

**Instructions :**
- CrÃ©er une matrice `A`
- CrÃ©er une copie `A_swapped` avec deux lignes Ã©changÃ©es
- Comparer `det(A)` et `det(A_swapped)`

**Fonctions utiles :**
- `np.copy`, slicing `[i], [j] = [j], [i]`, `np.linalg.det`

---

## Exercice 4 â€” Multiplication par une constante

**Ã‰noncÃ© :**  
VÃ©rifier que multiplier une ligne de `A` par `Î»` multiplie le dÃ©terminant par `Î»`.

**Instructions :**
- CrÃ©er une matrice `A`
- CrÃ©er une copie `A_scaled` oÃ¹ une ligne est multipliÃ©e par une constante `Î»`
- Comparer `det(A_scaled)` et `Î» * det(A)` (selon le rang de la matrice)

**Fonctions utiles :**
- `np.copy`, `np.linalg.det`, slicing

---

## Exercice 5 â€” DÃ©terminant dâ€™un produit matriciel

**Ã‰noncÃ© :**  
VÃ©rifier la propriÃ©tÃ© :  
```math
det(AB) = det(A) Â· det(B)
```
**Instructions :**
- CrÃ©er deux matrices carrÃ©es `A` et `B`
- Calculer `det(A) * det(B)` et `det(A @ B)`
- Comparer les deux rÃ©sultats

**Fonctions utiles :**
- `np.linalg.det`, `@`, `np.allclose`

---

# PCA â€” Application complÃ¨te (chapitre 2.12 final)

> ObjectifÂ : Appliquer une Analyse en Composantes Principales (PCA) sur des donnÃ©es 2D gÃ©nÃ©rÃ©es artificiellement. RÃ©duire la dimension dâ€™un jeu de donnÃ©es tout en conservant un maximum dâ€™information (variance, Ã  quel point les donnÃ©es s'Ã©cartent de la moyenne).

---

## ğŸ§ª DonnÃ©es

**GÃ©nÃ©ration des donnÃ©es simulÃ©es (100 points 2D)** :

```py
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

# GÃ©nÃ©ration de 100 points (introduisant une corrÃ©lation entre x et y)
X = np.random.randn(100, 2)
X[:, 1] = 2 * X[:, 0] + 0.5 * np.random.randn(100)

plt.scatter(X[:, 0], X[:, 1])
plt.title("Nuage de points initial")
plt.axis("equal")
plt.show()
```

---

## ğŸ“Œ Ã‰tape 1 â€” Centrer les donnÃ©es

**Ã‰noncÃ©Â :**
Centrer les donnÃ©es autour de zÃ©ro pour chaque axe (soustraire la moyenne).

**InstructionsÂ :**
- Calculer la moyenne des colonnes
- Soustraire cette moyenne de chaque point

**Fonctions utilesÂ :**
- `np.mean`, `axis=0`, broadcasting

---

## ğŸ“Œ Ã‰tape 2 â€” Calculer la matrice de covariance

**Ã‰noncÃ©Â :**

Calculer la matrice de covariance `Î£ = (1/n) * Xáµ€ @ X`

**InstructionsÂ :**
- Utiliser la version centrÃ©e de `X`
- Attention Ã  bien transposer avant multiplication

**Fonctions utilesÂ :**
- `np.dot`, `.T`, ou `@`

---

## ğŸ“Œ Ã‰tape 3 â€” DÃ©composer en vecteurs/vecteurs propres

**Ã‰noncÃ©Â :**
Appliquer lâ€™eigendecomposition de la matrice de covariance

**InstructionsÂ :**
- Utiliser `np.linalg.eigh` (symÃ©trique)
- Trier les valeurs propres par ordre dÃ©croissant

**Fonctions utilesÂ :**
- `np.linalg.eigh`, `np.argsort`, `[::-1]`

---

## ğŸ“Œ Ã‰tape 4 â€” RÃ©duire la dimension (1D)

**Ã‰noncÃ©Â :**
Projeter les donnÃ©es sur le 1er vecteur propre (plus grande valeur propre)

**InstructionsÂ :**
- Garder uniquement le premier vecteur propre `uâ‚`
- Calculer la projection : `X_proj = X @ uâ‚`

**Fonctions utilesÂ :**
- `np.dot`, `@`, slicing

---

## ğŸ“Œ Ã‰tape 5 â€” Visualisation

**Ã‰noncÃ©Â :**
Afficher les rÃ©sultats de la projection

**InstructionsÂ :**
- Reconstituer les points projetÃ©s dans lâ€™espace 2D pour visualiser la composante principale
- Tracer les vecteurs propres superposÃ©s au nuage initial (optionnel)

**Fonctions utilesÂ :**
- `matplotlib.pyplot.scatter`, `plt.quiver`, `plt.arrow`

---

## Objectif final

VÃ©rifier visuellement que :
- Le 1er vecteur propre suit la direction de plus grande variance
- La projection rÃ©duit correctement la dimension tout en capturant la structure